{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5dce05ac-9933-46ca-a72c-9dcc03946a1b",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "**Basic Description**\n",
    "\n",
    "Logisitc regression is one of the simplest and most popular classification algorithms. In logistic regression, a linear output is converted into a probability between 0 and 1 using the sigmoid function. Logistic regression is highly interprettable and easier to explain than other models.\n",
    "\n",
    "**Bias-Variance Tradeoff** \n",
    "\n",
    "Its simplicity makes for high bias and low variance.\n",
    "\n",
    "**Upsides**\n",
    "\n",
    "It's quick to implement and can serve as a good baseline for performance.\n",
    "\n",
    "**Downsides**\n",
    "\n",
    "It generally does not perform well with non-linear decision boundaries. When interprettability is desireably, it's important that features are not correlated.\n",
    "\n",
    "**Other Notes**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52affe8b",
   "metadata": {},
   "source": [
    "## Load Packages and Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5af3205a-9803-404a-a031-3ed432e1fc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom utils\n",
    "import utils\n",
    "print(utils.__file__)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import Lasso, Ridge, LogisticRegression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e46d3366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdual\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mfit_intercept\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mintercept_scaling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lbfgs'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmulti_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mwarm_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0ml1_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Logistic Regression (aka logit, MaxEnt) classifier.\n",
      "\n",
      "In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\n",
      "scheme if the 'multi_class' option is set to 'ovr', and uses the\n",
      "cross-entropy loss if the 'multi_class' option is set to 'multinomial'.\n",
      "(Currently the 'multinomial' option is supported only by the 'lbfgs',\n",
      "'sag', 'saga' and 'newton-cg' solvers.)\n",
      "\n",
      "This class implements regularized logistic regression using the\n",
      "'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note\n",
      "that regularization is applied by default**. It can handle both dense\n",
      "and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\n",
      "floats for optimal performance; any other input format will be converted\n",
      "(and copied).\n",
      "\n",
      "The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n",
      "with primal formulation, or no regularization. The 'liblinear' solver\n",
      "supports both L1 and L2 regularization, with a dual formulation only for\n",
      "the L2 penalty. The Elastic-Net regularization is only supported by the\n",
      "'saga' solver.\n",
      "\n",
      "Read more in the :ref:`User Guide <logistic_regression>`.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "penalty : {'l1', 'l2', 'elasticnet', 'none'}, default='l2'\n",
      "    Specify the norm of the penalty:\n",
      "\n",
      "    - `'none'`: no penalty is added;\n",
      "    - `'l2'`: add a L2 penalty term and it is the default choice;\n",
      "    - `'l1'`: add a L1 penalty term;\n",
      "    - `'elasticnet'`: both L1 and L2 penalty terms are added.\n",
      "\n",
      "    .. warning::\n",
      "       Some penalties may not work with some solvers. See the parameter\n",
      "       `solver` below, to know the compatibility between the penalty and\n",
      "       solver.\n",
      "\n",
      "    .. versionadded:: 0.19\n",
      "       l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n",
      "\n",
      "dual : bool, default=False\n",
      "    Dual or primal formulation. Dual formulation is only implemented for\n",
      "    l2 penalty with liblinear solver. Prefer dual=False when\n",
      "    n_samples > n_features.\n",
      "\n",
      "tol : float, default=1e-4\n",
      "    Tolerance for stopping criteria.\n",
      "\n",
      "C : float, default=1.0\n",
      "    Inverse of regularization strength; must be a positive float.\n",
      "    Like in support vector machines, smaller values specify stronger\n",
      "    regularization.\n",
      "\n",
      "fit_intercept : bool, default=True\n",
      "    Specifies if a constant (a.k.a. bias or intercept) should be\n",
      "    added to the decision function.\n",
      "\n",
      "intercept_scaling : float, default=1\n",
      "    Useful only when the solver 'liblinear' is used\n",
      "    and self.fit_intercept is set to True. In this case, x becomes\n",
      "    [x, self.intercept_scaling],\n",
      "    i.e. a \"synthetic\" feature with constant value equal to\n",
      "    intercept_scaling is appended to the instance vector.\n",
      "    The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
      "\n",
      "    Note! the synthetic feature weight is subject to l1/l2 regularization\n",
      "    as all other features.\n",
      "    To lessen the effect of regularization on synthetic feature weight\n",
      "    (and therefore on the intercept) intercept_scaling has to be increased.\n",
      "\n",
      "class_weight : dict or 'balanced', default=None\n",
      "    Weights associated with classes in the form ``{class_label: weight}``.\n",
      "    If not given, all classes are supposed to have weight one.\n",
      "\n",
      "    The \"balanced\" mode uses the values of y to automatically adjust\n",
      "    weights inversely proportional to class frequencies in the input data\n",
      "    as ``n_samples / (n_classes * np.bincount(y))``.\n",
      "\n",
      "    Note that these weights will be multiplied with sample_weight (passed\n",
      "    through the fit method) if sample_weight is specified.\n",
      "\n",
      "    .. versionadded:: 0.17\n",
      "       *class_weight='balanced'*\n",
      "\n",
      "random_state : int, RandomState instance, default=None\n",
      "    Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n",
      "    data. See :term:`Glossary <random_state>` for details.\n",
      "\n",
      "solver : {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},             default='lbfgs'\n",
      "\n",
      "    Algorithm to use in the optimization problem. Default is 'lbfgs'.\n",
      "    To choose a solver, you might want to consider the following aspects:\n",
      "\n",
      "        - For small datasets, 'liblinear' is a good choice, whereas 'sag'\n",
      "          and 'saga' are faster for large ones;\n",
      "        - For multiclass problems, only 'newton-cg', 'sag', 'saga' and\n",
      "          'lbfgs' handle multinomial loss;\n",
      "        - 'liblinear' is limited to one-versus-rest schemes.\n",
      "\n",
      "    .. warning::\n",
      "       The choice of the algorithm depends on the penalty chosen:\n",
      "       Supported penalties by solver:\n",
      "\n",
      "       - 'newton-cg'   -   ['l2', 'none']\n",
      "       - 'lbfgs'       -   ['l2', 'none']\n",
      "       - 'liblinear'   -   ['l1', 'l2']\n",
      "       - 'sag'         -   ['l2', 'none']\n",
      "       - 'saga'        -   ['elasticnet', 'l1', 'l2', 'none']\n",
      "\n",
      "    .. note::\n",
      "       'sag' and 'saga' fast convergence is only guaranteed on\n",
      "       features with approximately the same scale. You can\n",
      "       preprocess the data with a scaler from :mod:`sklearn.preprocessing`.\n",
      "\n",
      "    .. seealso::\n",
      "       Refer to the User Guide for more information regarding\n",
      "       :class:`LogisticRegression` and more specifically the\n",
      "       `Table <https://scikit-learn.org/dev/modules/linear_model.html#logistic-regression>`_\n",
      "       summarazing solver/penalty supports.\n",
      "\n",
      "    .. versionadded:: 0.17\n",
      "       Stochastic Average Gradient descent solver.\n",
      "    .. versionadded:: 0.19\n",
      "       SAGA solver.\n",
      "    .. versionchanged:: 0.22\n",
      "        The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n",
      "\n",
      "max_iter : int, default=100\n",
      "    Maximum number of iterations taken for the solvers to converge.\n",
      "\n",
      "multi_class : {'auto', 'ovr', 'multinomial'}, default='auto'\n",
      "    If the option chosen is 'ovr', then a binary problem is fit for each\n",
      "    label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
      "    across the entire probability distribution, *even when the data is\n",
      "    binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
      "    'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
      "    and otherwise selects 'multinomial'.\n",
      "\n",
      "    .. versionadded:: 0.18\n",
      "       Stochastic Average Gradient descent solver for 'multinomial' case.\n",
      "    .. versionchanged:: 0.22\n",
      "        Default changed from 'ovr' to 'auto' in 0.22.\n",
      "\n",
      "verbose : int, default=0\n",
      "    For the liblinear and lbfgs solvers set verbose to any positive\n",
      "    number for verbosity.\n",
      "\n",
      "warm_start : bool, default=False\n",
      "    When set to True, reuse the solution of the previous call to fit as\n",
      "    initialization, otherwise, just erase the previous solution.\n",
      "    Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n",
      "\n",
      "    .. versionadded:: 0.17\n",
      "       *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n",
      "\n",
      "n_jobs : int, default=None\n",
      "    Number of CPU cores used when parallelizing over classes if\n",
      "    multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n",
      "    set to 'liblinear' regardless of whether 'multi_class' is specified or\n",
      "    not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      "    context. ``-1`` means using all processors.\n",
      "    See :term:`Glossary <n_jobs>` for more details.\n",
      "\n",
      "l1_ratio : float, default=None\n",
      "    The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n",
      "    used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n",
      "    to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n",
      "    to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n",
      "    combination of L1 and L2.\n",
      "\n",
      "Attributes\n",
      "----------\n",
      "\n",
      "classes_ : ndarray of shape (n_classes, )\n",
      "    A list of class labels known to the classifier.\n",
      "\n",
      "coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
      "    Coefficient of the features in the decision function.\n",
      "\n",
      "    `coef_` is of shape (1, n_features) when the given problem is binary.\n",
      "    In particular, when `multi_class='multinomial'`, `coef_` corresponds\n",
      "    to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n",
      "\n",
      "intercept_ : ndarray of shape (1,) or (n_classes,)\n",
      "    Intercept (a.k.a. bias) added to the decision function.\n",
      "\n",
      "    If `fit_intercept` is set to False, the intercept is set to zero.\n",
      "    `intercept_` is of shape (1,) when the given problem is binary.\n",
      "    In particular, when `multi_class='multinomial'`, `intercept_`\n",
      "    corresponds to outcome 1 (True) and `-intercept_` corresponds to\n",
      "    outcome 0 (False).\n",
      "\n",
      "n_features_in_ : int\n",
      "    Number of features seen during :term:`fit`.\n",
      "\n",
      "    .. versionadded:: 0.24\n",
      "\n",
      "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "    Names of features seen during :term:`fit`. Defined only when `X`\n",
      "    has feature names that are all strings.\n",
      "\n",
      "    .. versionadded:: 1.0\n",
      "\n",
      "n_iter_ : ndarray of shape (n_classes,) or (1, )\n",
      "    Actual number of iterations for all classes. If binary or multinomial,\n",
      "    it returns only 1 element. For liblinear solver, only the maximum\n",
      "    number of iteration across all classes is given.\n",
      "\n",
      "    .. versionchanged:: 0.20\n",
      "\n",
      "        In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n",
      "        ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "SGDClassifier : Incrementally trained logistic regression (when given\n",
      "    the parameter ``loss=\"log\"``).\n",
      "LogisticRegressionCV : Logistic regression with built-in cross validation.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "The underlying C implementation uses a random number generator to\n",
      "select features when fitting the model. It is thus not uncommon,\n",
      "to have slightly different results for the same input data. If\n",
      "that happens, try with a smaller tol parameter.\n",
      "\n",
      "Predict output may not match that of standalone liblinear in certain\n",
      "cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
      "in the narrative documentation.\n",
      "\n",
      "References\n",
      "----------\n",
      "\n",
      "L-BFGS-B -- Software for Large-scale Bound-constrained Optimization\n",
      "    Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\n",
      "    http://users.iems.northwestern.edu/~nocedal/lbfgsb.html\n",
      "\n",
      "LIBLINEAR -- A Library for Large Linear Classification\n",
      "    https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n",
      "\n",
      "SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n",
      "    Minimizing Finite Sums with the Stochastic Average Gradient\n",
      "    https://hal.inria.fr/hal-00860051/document\n",
      "\n",
      "SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n",
      "        :arxiv:`\"SAGA: A Fast Incremental Gradient Method With Support\n",
      "        for Non-Strongly Convex Composite Objectives\" <1407.0202>`\n",
      "\n",
      "Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n",
      "    methods for logistic regression and maximum entropy models.\n",
      "    Machine Learning 85(1-2):41-75.\n",
      "    https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> from sklearn.datasets import load_iris\n",
      ">>> from sklearn.linear_model import LogisticRegression\n",
      ">>> X, y = load_iris(return_X_y=True)\n",
      ">>> clf = LogisticRegression(random_state=0).fit(X, y)\n",
      ">>> clf.predict(X[:2, :])\n",
      "array([0, 0])\n",
      ">>> clf.predict_proba(X[:2, :])\n",
      "array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n",
      "       [9.7...e-01, 2.8...e-02, ...e-08]])\n",
      ">>> clf.score(X, y)\n",
      "0.97...\n",
      "\u001b[0;31mFile:\u001b[0m           ~/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     LogisticRegressionCV\n"
     ]
    }
   ],
   "source": [
    "LogisticRegression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39b0d866-7f32-4702-be00-b1d42f42f864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (62889, 42)\n",
      "y_train (62889,)\n",
      "X_test (15723, 42)\n",
      "y_test (15723,)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "X_train, y_train, X_test, y_test = utils.load_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "774e622f",
   "metadata": {},
   "source": [
    "## Model 1\n",
    "- Default hyperparameters\n",
    "- Notable\n",
    "    - penalty='l2'\n",
    "    - C=1.0\n",
    "    - max_iter=100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22a9b5e7-e23e-490e-817a-dd02c9c5c7a2",
   "metadata": {},
   "source": [
    "### Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "950c7dfc-273c-4002-a257-23dee3579c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit logistic regression model\n",
    "log_1 = LogisticRegression(n_jobs=-1)\n",
    "x = log_1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd218da-7182-4182-afb1-b2d0ae8ac2db",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca34a73a-0f85-45cf-8544-e5f5c04d4321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5198 0.5028 0.4952 0.4939 0.4959]\n",
      "0.5015\n"
     ]
    }
   ],
   "source": [
    "# cross validation with f1 scoring\n",
    "score = utils.f1_cv(log_1, X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46fcc8dc",
   "metadata": {},
   "source": [
    "## Model X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb80fa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_x = LogisticRegression(penalty='l1', C=0.1, solver='liblinear')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "57ab2a48",
   "metadata": {},
   "source": [
    "## Model 2\n",
    "- Regularize by selecting important features from recursive feature elimination ranking"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef8da07c",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6676f1f-ca9b-4cec-bd5d-4fa577e91494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recursive feature elimination to determine feature importance\n",
    "log = LogisticRegression(max_iter = 200)\n",
    "model_rfe = RFECV(log, cv = 5)\n",
    "x = model_rfe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e7565a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/nl/d1z3_54n5kb053_jgq9kvjnh0000gn/T/ipykernel_26585/4076918754.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mrfe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_rfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mranking_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mrfe_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'features'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rfe_rank'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrfe\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mrfe_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'rfe_rank'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# feature ranking\n",
    "rfe = model_rfe.ranking_\n",
    "features = X_train.columns\n",
    "rfe_df = pd.DataFrame({'features': features, 'rfe_rank': rfe})\n",
    "rfe_df.sort_values(by = 'rfe_rank', ascending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d01dbae-ff36-4e9b-b686-9e6ea3c18b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features ranked 1 are the most important\n",
    "# select only the more important features as a means of regularization\n",
    "selected_features = rfe_df[rfe_df['rfe_rank'] == 1]['features'].values\n",
    "X_train_new = X_train[selected_features]\n",
    "X_test_new = X_test[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb7a68f-66f6-4bb6-bf95-39323fa5d864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62889, 16)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 16 important features\n",
    "X_train_new.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22a9b5e7-e23e-490e-817a-dd02c9c5c7a2",
   "metadata": {},
   "source": [
    "### Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950c7dfc-273c-4002-a257-23dee3579c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit logistic regression model\n",
    "# using max_iter 400 to avoid error message\n",
    "# using X_train_new to select only the 16 features that were selected\n",
    "log_2 = LogisticRegression(max_iter = 400)\n",
    "x = log_2.fit(X_train_new, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd218da-7182-4182-afb1-b2d0ae8ac2db",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca34a73a-0f85-45cf-8544-e5f5c04d4321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5198 0.5028 0.4952 0.4939 0.4959]\n",
      "0.5015\n"
     ]
    }
   ],
   "source": [
    "# cross validation with f1 scoring\n",
    "score = utils.f1_cv(log_2, X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9485863a-143d-4c0c-adfb-d2e3e35a03c1",
   "metadata": {},
   "source": [
    "## Model 3\n",
    "- Adjust hyperparameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "490b68ad",
   "metadata": {},
   "source": [
    "### Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecc11b2-a063-4498-8fee-0046f058e98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tried a variety of different combinations of parameters\n",
    "# same selected features as the original model\n",
    "log_3 = LogisticRegression(max_iter = 600, penalty=\"l1\", tol=0.001, solver=\"saga\")\n",
    "x = log_3.fit(X_train_new, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8d32a3b6",
   "metadata": {},
   "source": [
    "### Cross-Validation\n",
    "- Slight improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef222b0-4a65-4810-9ccd-2ebdb787d12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5588 0.5344 0.5327 0.5215 0.535 ]\n",
      "0.5365\n"
     ]
    }
   ],
   "source": [
    "# cross validation with f1 scoring\n",
    "score = utils.f1_cv(log_3, X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad7aa2c7-d77c-4af1-8a48-a023adbb6b53",
   "metadata": {},
   "source": [
    "## Test Performance\n",
    "Final test performance of model chosen using best cross-validation scores. These scores will be used to select amongst different model types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a62606-f737-4d8c-9016-8676017ba1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\t0.9452394581186796\n",
      "Precision:\t0.82\n",
      "Recall:\t\t0.2645161290322581\n",
      "F1:\t\t0.4\n"
     ]
    }
   ],
   "source": [
    "# test the performance of the selected model\n",
    "y_pred = log_3.predict(X_test_new)\n",
    "\n",
    "utils.pred_metrics(y_test,y_pred)\n",
    "\n",
    "# confusion matrix\n",
    "utils.cm_plot(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1558fe3c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 18:29:29) \n[Clang 12.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "9ce51b511e31678062ea377e4f294f92e7c7692784d9643ac534d9eb5246e9c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
